{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86496c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    Docx2txtLoader,\n",
    "    CSVLoader,\n",
    "    UnstructuredMarkdownLoader\n",
    ")\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128e5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d443776",
   "metadata": {},
   "source": [
    "So now we are uploading the Documnets and storing it in a forlder Uploaded Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315b2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_upload_directory(upload_dir=\"uploaded_documents\"):\n",
    "    \"\"\"\n",
    "    Create the upload directory if it doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        upload_dir (str): Directory where uploaded documents will be stored\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the created directory\n",
    "    \"\"\"\n",
    "    Path(upload_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Upload directory ready: {upload_dir}\")\n",
    "    return upload_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4caa8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_document(file_path, upload_dir=\"uploaded_documents\"):\n",
    "    \"\"\"\n",
    "    Upload a document to the upload directory\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to upload\n",
    "        upload_dir (str): Directory where the file will be uploaded\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the uploaded file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    create_upload_directory(upload_dir)\n",
    "    \n",
    "    file_name = os.path.basename(file_path)\n",
    "    destination = os.path.join(upload_dir, file_name)\n",
    "    \n",
    "    # Copy file to upload directory\n",
    "    shutil.copy2(file_path, destination)\n",
    "    \n",
    "    print(f\"Document uploaded successfully: {destination}\")\n",
    "    return destination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9058e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    \"\"\"\n",
    "    Load a document using the appropriate LangChain loader\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the document\n",
    "        \n",
    "    Returns:\n",
    "        list: List of Document objects\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    file_extension = Path(file_path).suffix.lower()\n",
    "    \n",
    "    loaders = {\n",
    "        '.pdf': PyPDFLoader,\n",
    "        '.txt': TextLoader,\n",
    "        '.docx': Docx2txtLoader,\n",
    "        '.csv': CSVLoader,\n",
    "        '.md': UnstructuredMarkdownLoader\n",
    "    }\n",
    "    \n",
    "    loader_class = loaders.get(file_extension)\n",
    "    \n",
    "    if not loader_class:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "    print(f\"Loading document with {loader_class.__name__}...\")\n",
    "    loader = loader_class(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} document(s)\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd91cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload directory ready: uploaded_documents\n",
      "Document uploaded successfully: uploaded_documents\\HR_FAQs_Keeper_Program.pdf\n",
      "Loading document with PyPDFLoader...\n",
      "Loaded 4 document(s)\n",
      "Loaded 4 document(s)\n"
     ]
    }
   ],
   "source": [
    "import pypdf\n",
    "\n",
    "# Prompt user for file path\n",
    "file_path = input(\"Enter the path to the document you want to upload: \")\n",
    "\n",
    "uploaded_path = upload_document(\n",
    "    file_path=file_path,\n",
    ")\n",
    "\n",
    "docs = load_document(uploaded_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3912a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "#     \"\"\"\n",
    "#     Split documents into smaller chunks using fixed-size chunking\n",
    "    \n",
    "#     Args:\n",
    "#         documents (list): List of Document objects\n",
    "#         chunk_size (int): Size of each chunk\n",
    "#         chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of split Document objects with preserved metadata\n",
    "#     \"\"\"\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         length_function=len,\n",
    "#         add_start_index=True  # Adds character index to metadata\n",
    "#     )\n",
    "    \n",
    "#     split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "#     # Add additional metadata to each chunk\n",
    "#     for i, doc in enumerate(split_docs):\n",
    "#         doc.metadata['chunk_id'] = i\n",
    "#         doc.metadata['chunk_size'] = len(doc.page_content)\n",
    "#         doc.metadata['chunking_method'] = 'fixed_size'\n",
    "#         doc.metadata['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "#     print(f\"Split into {len(split_docs)} chunks using fixed-size chunking\")\n",
    "#     return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428a06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_split_documents(documents, embeddings_model=None, breakpoint_threshold_type=\"percentile\"):\n",
    "    \"\"\"\n",
    "    Split documents using semantic chunking (groups text by meaning)\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects\n",
    "        embeddings_model: Embeddings model to use (default: OpenAIEmbeddings)\n",
    "        breakpoint_threshold_type (str): \"percentile\", \"standard_deviation\", or \"interquartile\"\n",
    "        \n",
    "    Returns:\n",
    "        list: List of semantically split Document objects with metadata\n",
    "    \"\"\"\n",
    "    if embeddings_model is None:\n",
    "        # You can replace this with other embedding models\n",
    "        embeddings_model = OpenAIEmbeddings()\n",
    "    \n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings_model,\n",
    "        breakpoint_threshold_type=breakpoint_threshold_type\n",
    "    )\n",
    "    \n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    for i, doc in enumerate(split_docs):\n",
    "        doc.metadata['chunk_id'] = i\n",
    "        doc.metadata['chunk_size'] = len(doc.page_content)\n",
    "        doc.metadata['chunking_method'] = 'semantic'\n",
    "        doc.metadata['breakpoint_type'] = breakpoint_threshold_type\n",
    "        doc.metadata['timestamp'] = datetime.now().isoformat()\n",
    "        doc.metadata['page'] = doc.metadata.get('page', 'unknown')\n",
    "    \n",
    "    print(f\"Split into {len(split_docs)} chunks using semantic chunking\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a9d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_metadata(chunk):\n",
    "    \"\"\"\n",
    "    Display all metadata for a specific chunk\n",
    "    \n",
    "    Args:\n",
    "        chunk: A Document object\n",
    "        \n",
    "    Returns:\n",
    "        dict: Metadata dictionary\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Chunk Metadata ---\")\n",
    "    for key, value in chunk.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return chunk.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7891fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 11 chunks using semantic chunking\n",
      "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2025-05-05T15:36:39+00:00', 'author': 'python-docx', 'moddate': '2025-05-05T15:36:39+00:00', 'source': 'uploaded_documents\\\\HR_FAQs_Keeper_Program.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'chunk_id': 0, 'chunk_size': 568, 'chunking_method': 'semantic', 'breakpoint_type': 'percentile', 'timestamp': '2025-11-29T21:05:40.368159'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load document\n",
    "\n",
    "# Semantic chunking (groups by meaning)\n",
    "chunks = semantic_split_documents(docs, embeddings_model=OpenAIEmbeddings())\n",
    "\n",
    "# Check metadata\n",
    "print(chunks[0].metadata)\n",
    "# Output includes: chunk_id, chunk_size, chunking_method, breakpoint_type, timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e5c4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def store_embeddings_in_chroma(chunks, embeddings_model=None, persist_directory=\"chroma\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the document chunks and stores them in Chroma DB.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): List of Document objects (chunks of text).\n",
    "        embeddings_model: Embeddings model to use (default: OpenAIEmbeddings).\n",
    "        persist_directory (str): Directory to persist the Chroma DB.\n",
    "        \n",
    "    Returns:\n",
    "        chromadb.Client: Chroma DB client.\n",
    "    \"\"\"\n",
    "    if embeddings_model is None:\n",
    "        embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "    # Extract text from document chunks\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "    # Create Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "    # Create a collection in Chroma\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"my_collection\",  # You can change the collection name\n",
    "        embedding_function=embedding_functions.OpenAIEmbeddingFunction(\n",
    "            model_name=\"text-embedding-ada-002\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add embeddings and text to Chroma\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=texts,\n",
    "        ids=[str(i) for i in range(len(chunks))]  # Unique IDs for each chunk\n",
    "    )\n",
    "\n",
    "    print(f\"Stored {len(chunks)} embeddings in Chroma DB at {persist_directory}\")\n",
    "    return chroma_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f79abd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 11 embeddings in Chroma DB at chroma\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Call the function to store embeddings in Chroma\n",
    "chroma_client = store_embeddings_in_chroma(chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36a33dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def save_metadata_to_sqlite(\n",
    "    documents,\n",
    "    db_path=\"metadata.db\",\n",
    "    table_name=\"documents_metadata\",\n",
    "    preview_chars=300\n",
    "):\n",
    "    \"\"\"\n",
    "    Save metadata from LangChain Document objects into SQLite.\n",
    "    Stores: id, page_number, chunk_id, chunk_size, chunking_method, timestamp, preview, full metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create table with page number column\n",
    "    cur.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            page_number INTEGER,\n",
    "            chunk_id INTEGER,\n",
    "            chunk_size INTEGER,\n",
    "            chunking_method TEXT,\n",
    "            timestamp TEXT,\n",
    "            text_preview TEXT,\n",
    "            metadata_json TEXT\n",
    "        );\n",
    "    \"\"\")\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Stable ID\n",
    "        doc_id = doc.metadata.get(\"id\") or str(uuid.uuid4())\n",
    "        doc.metadata[\"id\"] = doc_id\n",
    "        ids.append(doc_id)\n",
    "\n",
    "        # Extract fields\n",
    "        page_number = doc.metadata.get(\"page\", None)           # <-- NEW  \n",
    "        chunk_id = doc.metadata.get(\"chunk_id\")\n",
    "        chunk_size = doc.metadata.get(\"chunk_size\", len(doc.page_content))\n",
    "        chunking_method = doc.metadata.get(\"chunking_method\")\n",
    "        timestamp = doc.metadata.get(\"timestamp\")\n",
    "        text_preview = doc.page_content[:preview_chars]\n",
    "\n",
    "        # Save full metadata JSON\n",
    "        metadata_json = json.dumps(doc.metadata, ensure_ascii=False)\n",
    "\n",
    "        cur.execute(\n",
    "            f\"\"\"\n",
    "            INSERT OR REPLACE INTO {table_name} \n",
    "            (id, page_number, chunk_id, chunk_size, chunking_method, timestamp, text_preview, metadata_json)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                doc_id,\n",
    "                page_number,       # <-- NEW\n",
    "                chunk_id,\n",
    "                chunk_size,\n",
    "                chunking_method,\n",
    "                timestamp,\n",
    "                text_preview,\n",
    "                metadata_json\n",
    "            )\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Saved metadata for {len(documents)} chunks → {db_path}:{table_name}\")\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d219e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metadata for 11 chunks → uploaded_documents/metadata.db:documents_metadata\n"
     ]
    }
   ],
   "source": [
    "# Save metadata to SQLite\n",
    "ids = save_metadata_to_sqlite(\n",
    "    documents=chunks,\n",
    "    db_path=\"uploaded_documents/metadata.db\",\n",
    "    table_name=\"documents_metadata\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f60459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Metadata not found for chunk ID 10\n",
      "Warning: Metadata not found for chunk ID 4\n",
      "Warning: Metadata not found for chunk ID 0\n",
      "Content: What if our finance or legal team needs to review this? We share a pre-drafted Tripartite Agreement vetted by multiple legal teams. It clearly outlines \n",
      "roles, data sharing, indemnity, and operational boundaries. 20. Who else is using Keeper? Companies like ABP Network, Ditto Insurance, Noora Health, and Progcap are already using the \n",
      "platform. We’re backed by Antler and early investors in Zomato, Swiggy, and Jupiter. For any additional questions, Keeper’s legal, tech, and operations teams are happy to support 1:1 \n",
      "with HR, Finance, or Legal.\n",
      "Metadata: {'id': '10'}\n",
      "---\n",
      "Content: Very minimal. Keeper handles communication, onboarding, loan servicing, and support. Deductions are requested only when needed. We also integrate with your HRMS (if available) to \n",
      "automate data sync. 8. What data does Keeper access, and how is it protected? Keeper accesses limited fields: employee name, ID, contact, DOJ, salary, and leave balance. All \n",
      "data is encrypted, stored securely, and accessed only as per the Tripartite Agreement. We comply \n",
      "with Indian data privacy regulations and provide indemnity for any misuse. 9. Can employees misuse or over-borrow? No. Employees can only avail an amount equivalent to their paid leave value. NBFC partners \n",
      "apply credit scoring, FOIR checks (max 60%), and income verification. Disbursal is subject to \n",
      "underwriting approval.\n",
      "Metadata: {'id': '4'}\n",
      "---\n",
      "Content: HR FAQs for Keeper's Paid Leave \n",
      "Advance Program  \n",
      "Below is a comprehensive list of questions HR leaders often ask after the first meeting with \n",
      "Keeper, along with detailed and structured responses to help in internal alignment and decision-\n",
      "making. --- \n",
      " \n",
      "1. What exactly is Keeper offering? Keeper facilitates an advance for employees against their accrued paid leaves via a licensed \n",
      "NBFC partner. The advance amount is limited to the monetizable value of the leave balance. It is \n",
      "not a loan from the employer or a salary advance, and no policy changes are needed.\n",
      "Metadata: {'id': '0'}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# ... previous code ...\n",
    "\n",
    "import sqlite3\n",
    "import chromadb\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def similarity_search_with_metadata(query, chroma_path=\"chroma\", sqlite_path=\"uploaded_documents/metadata.db\", table_name=\"documents_metadata\", top_k=2, embeddings_model=None):\n",
    "    \"\"\"\n",
    "    Perform a similarity search in ChromaDB and augment results with metadata from SQLite.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        chroma_path (str): Path to the persisted Chroma database.\n",
    "        sqlite_path (str): Path to the SQLite database containing document metadata.\n",
    "        table_name (str): Name of the table in the SQLite database.\n",
    "        top_k (int): Number of results to return.\n",
    "        embeddings_model: Embeddings model to use for similarity search (default: OpenAIEmbeddings).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Document objects, each containing the chunk content and metadata from both Chroma and SQLite.\n",
    "    \"\"\"\n",
    "\n",
    "    if embeddings_model is None:\n",
    "        embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "    # Initialize Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=\"my_collection\",  # Ensure this matches the collection name used during storage\n",
    "        embedding_function=chromadb.utils.embedding_functions.OpenAIEmbeddingFunction(\n",
    "            model_name=\"text-embedding-ada-002\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    embedding = embeddings_model.embed_query(query)\n",
    "\n",
    "    # Perform similarity search in Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=[embedding],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\"],\n",
    "    )\n",
    "\n",
    "    # Extract relevant information from Chroma results\n",
    "    ids = results[\"ids\"][0]  # Chunk IDs from Chroma\n",
    "    documents = results[\"documents\"][0]  # Chunk contents\n",
    "\n",
    "    # Fetch metadata from SQLite\n",
    "    conn = sqlite3.connect(sqlite_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Prepare the IN clause for the SQL query\n",
    "    placeholders = ', '.join('?' for _ in ids)\n",
    "    sql_query = f\"SELECT * FROM {table_name} WHERE id IN ({placeholders})\"\n",
    "\n",
    "    cursor.execute(sql_query, ids)\n",
    "    metadata_rows = cursor.fetchall()\n",
    "    column_names = [description[0] for description in cursor.description]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Create a dictionary mapping ID to metadata for easy lookup\n",
    "    metadata_dict = {row[0]: dict(zip(column_names, row)) for row in metadata_rows}\n",
    "\n",
    "    # Combine chunk content with metadata\n",
    "    combined_results = []\n",
    "    for i, doc_id in enumerate(ids):\n",
    "        metadata = metadata_dict.get(doc_id)\n",
    "        if metadata:\n",
    "            combined_metadata = metadata  # Use all metadata from SQLite\n",
    "            combined_results.append(\n",
    "                Document(\n",
    "                    page_content=documents[i],\n",
    "                    metadata=combined_metadata\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Metadata not found for chunk ID {doc_id}\")\n",
    "            # Still create a Document, but with limited info\n",
    "            combined_results.append(\n",
    "                Document(page_content=documents[i], metadata={\"id\": doc_id})\n",
    "            )\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "# Example usage:\n",
    "results = similarity_search_with_metadata(\"What is keeper?\", top_k=3)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Content: {result.page_content}\\nMetadata: {result.metadata}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619757eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... previous imports ...\n",
    "\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ... previous code ...\n",
    "\n",
    "def generate_answer(system_prompt, chunks, question, history=None, model_name=\"gpt-3.5-turbo\", max_history=5):\n",
    "    system_prompt = \"\"\"You are a helpful assistant that answers questions about the document. You must answer strictly based on the context provided in the documents. If the answer is not contained within the text below, say \"I don't know\". \"\"\"\n",
    "\n",
    "    # Initialize chat model\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=0.1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # ✅ NEW MEMORY IMPLEMENTATION\n",
    "    # -----------------------------\n",
    "    # Use InMemoryChatMessageHistory to store conversation state\n",
    "    chat_memory = InMemoryChatMessageHistory()\n",
    "\n",
    "    # Seed existing history if provided (list of HumanMessage/AIMessage)\n",
    "    if history:\n",
    "        for msg in history:\n",
    "            chat_memory.add_message(msg)\n",
    "\n",
    "    # Load existing messages for prompt\n",
    "    chat_history_messages = chat_memory.messages\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare RAG context\n",
    "    # -----------------------------\n",
    "    context = \"\\n\".join([f\"{chunk.page_content}\" for chunk in chunks])\n",
    "\n",
    "    # Build prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt + \"\\n\\nContext:\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{question}\")\n",
    "    ])\n",
    "    # Build LCEL chain\n",
    "    chain = (\n",
    "        {\"context\": (lambda x: context), \"question\": (lambda x: question), \"chat_history\": (lambda x: chat_history_messages)}\n",
    "        | prompt\n",
    "        | llm.bind(stop=[\"\\nUser\"])\n",
    "    )\n",
    "\n",
    "    # Run Model\n",
    "    response = chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": chat_history_messages\n",
    "    })\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Save new interaction to chat memory\n",
    "    # ------------------------------------\n",
    "    chat_memory.add_user_message(question)\n",
    "    chat_memory.add_ai_message(response.content)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Trim history to last N turns\n",
    "    # Each turn = 2 messages (Human + AI)\n",
    "    # ------------------------------------\n",
    "    trimmed = chat_memory.messages[-(max_history * 2):]\n",
    "\n",
    "    return response.content, trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c434716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Metadata not found for chunk ID 0\n",
      "Warning: Metadata not found for chunk ID 4\n",
      "Warning: Metadata not found for chunk ID 8\n",
      "--- Retrieved Chunks ---\n",
      "Chunk 1 Content:\n",
      "HR FAQs for Keeper's Paid Leave \n",
      "Advance Program  \n",
      "Below is a comprehensive list of questions HR leaders often ask after the first meeting with \n",
      "Keeper, along with detailed and structured responses to help in internal alignment and decision-\n",
      "making. --- \n",
      " \n",
      "1. What exactly is Keeper offering? Keeper facilitates an advance for employees against their accrued paid leaves via a licensed \n",
      "NBFC partner. The advance amount is limited to the monetizable value of the leave balance. It is \n",
      "not a loan from the employer or a salary advance, and no policy changes are needed.\n",
      "Metadata: {'id': '0'}\n",
      "---\n",
      "Chunk 2 Content:\n",
      "Very minimal. Keeper handles communication, onboarding, loan servicing, and support. Deductions are requested only when needed. We also integrate with your HRMS (if available) to \n",
      "automate data sync. 8. What data does Keeper access, and how is it protected? Keeper accesses limited fields: employee name, ID, contact, DOJ, salary, and leave balance. All \n",
      "data is encrypted, stored securely, and accessed only as per the Tripartite Agreement. We comply \n",
      "with Indian data privacy regulations and provide indemnity for any misuse. 9. Can employees misuse or over-borrow? No. Employees can only avail an amount equivalent to their paid leave value. NBFC partners \n",
      "apply credit scoring, FOIR checks (max 60%), and income verification. Disbursal is subject to \n",
      "underwriting approval.\n",
      "Metadata: {'id': '4'}\n",
      "---\n",
      "Chunk 3 Content:\n",
      "What if employees are not interested in using it? There’s no impact. The program is 100% voluntary. Only interested and eligible employees are \n",
      "onboarded. 18. Do we need to sign agreements with each employee?\n",
      "Metadata: {'id': '8'}\n",
      "---\n",
      "Question: Can you let me know the first question I asked?\n",
      "Answer: The first question you asked was, \"What exactly is Keeper offering?\"\n",
      "Question: Can you let me know the first question I asked?\n",
      "Answer: The first question you asked was, \"What exactly is Keeper offering?\"\n"
     ]
    }
   ],
   "source": [
    "# Debugging cell: Check retrieval and LLM context\n",
    "question = \"Can you let me know the first question I asked?\"\n",
    "results = similarity_search_with_metadata(question, top_k=3)\n",
    "\n",
    "print(\"--- Retrieved Chunks ---\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Chunk {i+1} Content:\\n{result.page_content}\\nMetadata: {result.metadata}\\n---\")\n",
    "\n",
    "# Pass retrieved chunks to LLM\n",
    "answer, history = generate_answer(\n",
    "    system_prompt=\"You are a helpful assistant that answers questions about the document. You must answer strictly based on the context provided in the documents. If the answer is not contained within the text below, say 'I don't know'. \",\n",
    "    chunks=results,\n",
    "    question=question\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")# Removed duplicate print statement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
